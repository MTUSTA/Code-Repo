{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Timer starts\n",
    "a = datetime.datetime.now()\n",
    "\n",
    "\n",
    "def n_gram_function(data, ng_input, element):\n",
    "    # CountVectorizer()is a object, dont delete object variable name\n",
    "    global n_gram\n",
    "    n_gram = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(ng_input, ng_input), min_df=20, encoding=\"utf-8\")\n",
    "    x1 = n_gram.fit_transform(data[element].values)\n",
    "    # matrix form\n",
    "    return x1\n",
    "\n",
    "\n",
    "# print 3 words(highest frequency)\n",
    "def most_3(frequency):\n",
    "    # sorted from small to large\n",
    "    sorted_unigram_g0 = np.argsort(frequency, kind='heapsort', axis=None)\n",
    "    sum_of_train_unigram_g0_frekans = np.sum(sorted_unigram_g0)\n",
    "\n",
    "    # Frequency / Numbers of All Datas\n",
    "    print(n_gram.get_feature_names()[sorted_unigram_g0.item(-1)], \"= %\", frequency.item(sorted_unigram_g0.item(-1))/sum_of_train_unigram_g0_frekans,\n",
    "          n_gram.get_feature_names()[sorted_unigram_g0.item(-2)], \"= %\", frequency.item(sorted_unigram_g0.item(-2))/sum_of_train_unigram_g0_frekans,\n",
    "          n_gram.get_feature_names()[sorted_unigram_g0.item(-3)], \"= %\", frequency.item(sorted_unigram_g0.item(-3))/sum_of_train_unigram_g0_frekans)\n",
    "\n",
    "\n",
    "def part1(data):\n",
    "    # grouped by title and label\n",
    "    grouped_Data = data[[\"title\", \"label\"]]\n",
    "    # 0 is real 1 is fake\n",
    "    group0 = grouped_Data[grouped_Data.label == 0]\n",
    "    group1 = grouped_Data[grouped_Data.label == 1]\n",
    "\n",
    "    # real unigram,bigram\n",
    "    print(\"Specific 3 title keywords of unigram real group\")\n",
    "    unigram_g0 = n_gram_function(group0, 1, \"title\")\n",
    "    unigram_g0_frekans = unigram_g0.sum(axis=0)\n",
    "    most_3(unigram_g0_frekans)\n",
    "    print(\"Specific 3 title keywords of bigram real group\")\n",
    "    bigram_g0 = n_gram_function(group0, 2, \"title\")\n",
    "    bigram_g0_frekans = bigram_g0.sum(axis=0)\n",
    "    most_3(bigram_g0_frekans)\n",
    "\n",
    "    # fake unigram,bigram\n",
    "    print(\"Specific 3 title keywords of unigram fake group\")\n",
    "    unigram_g1 = n_gram_function(group1, 1, \"title\")\n",
    "    unigram_g1_frekans = unigram_g1.sum(axis=0)\n",
    "    most_3(unigram_g1_frekans)\n",
    "    print(\"Specific 3 title keywords of bigram fake group\")\n",
    "    bigram_g1 = n_gram_function(group1, 2, \"title\")\n",
    "    bigram_g1_frekans = bigram_g1.sum(axis=0)\n",
    "    most_3(bigram_g1_frekans)\n",
    "\n",
    "# P(w|c) = (count(w,c)+1) / count(c)+ unique words size\n",
    "def sum_frequency(freq, all_freq, data):\n",
    "    return (freq + 1) / (all_freq + data)\n",
    "\n",
    "\n",
    "def n_gram_function2(data, ng_input):\n",
    "    # CountVectorizer()is a object, dont delete object variable name\n",
    "    global n_gram\n",
    "    try:\n",
    "        n_gram = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(ng_input, ng_input), encoding=\"utf-8\")\n",
    "        x1 = n_gram.fit_transform(data)\n",
    "        # matrix form\n",
    "        return x1\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def part2(data):\n",
    "    # split data randomly\n",
    "    test_data = data.sample(n=len(data) // 5, random_state=1)\n",
    "    # remove a pandas dataframe from another dataframe\n",
    "    # remove test data from main(all) data = train data\n",
    "    train_data = pd.concat([data, test_data, test_data]).drop_duplicates(keep=False)\n",
    "\n",
    "    # group data by text and label(fake or real)\n",
    "    train_grouped_data = train_data[[\"text\", \"label\"]]\n",
    "\n",
    "    # group by 0 (is real) or 1 (is fake)\n",
    "    train_group0 = train_grouped_data[train_grouped_data.label == 0]\n",
    "    train_group1 = train_grouped_data[train_grouped_data.label == 1]\n",
    "\n",
    "    # priors len fake / all train data or len real / all train data\n",
    "    prior_real = len(train_group0) / (len(train_group1) + len(train_group0))\n",
    "    prior_fake = len(train_group1) / (len(train_group1) + len(train_group0))\n",
    "\n",
    "\n",
    "    # real unigram for train data\n",
    "    train_unigram_g0 = n_gram_function(train_group0, 1, \"text\")\n",
    "    # frequency of every word\n",
    "    train_unigram_g0_frekans = train_unigram_g0.sum(axis=0)\n",
    "    # sum of frequencies\n",
    "    sum_of_train_unigram_g0_frekans = np.sum(train_unigram_g0_frekans)\n",
    "    # words\n",
    "    train_unigram_g0_feature = n_gram.get_feature_names()\n",
    "\n",
    "    # real bigram for train data\n",
    "    train_bigram_g0 = n_gram_function(train_group0, 2, \"text\")\n",
    "    # frequency of every word\n",
    "    train_bigram_g0_frekans = train_bigram_g0.sum(axis=0)\n",
    "    # sum of frequencies\n",
    "    sum_of_train_bigram_g0_frekans = np.sum(train_bigram_g0_frekans)\n",
    "    # words\n",
    "    train_bigram_g0_feature = n_gram.get_feature_names()\n",
    "\n",
    "    # fake unigram for train data\n",
    "    train_unigram_g1 = n_gram_function(train_group1, 1, \"text\")\n",
    "    # frequency of every word\n",
    "    train_unigram_g1_frekans = train_unigram_g1.sum(axis=0)\n",
    "    # sum of frequencies\n",
    "    sum_of_train_unigram_g1_frekans = np.sum(train_unigram_g1_frekans)\n",
    "    # words\n",
    "    train_unigram_g1_feature = n_gram.get_feature_names()\n",
    "\n",
    "    # fake bigram for train data\n",
    "    train_bigram_g1 = n_gram_function(train_group1, 2, \"text\")\n",
    "    # frequency of every word\n",
    "    train_bigram_g1_frekans = train_bigram_g1.sum(axis=0)\n",
    "    # sum of frequencies\n",
    "    sum_of_train_bigram_g1_frekans = np.sum(train_bigram_g1_frekans)\n",
    "    # words\n",
    "    train_bigram_g1_feature = n_gram.get_feature_names()\n",
    "\n",
    "\n",
    "    test_grouped_Data = test_data[[\"text\", \"label\"]]\n",
    "    # 0 is real 1 is fake\n",
    "    len_train_g0_frekans = len(train_unigram_g0_feature)\n",
    "    len_train_g1_frekans = len(train_unigram_g1_feature)\n",
    "\n",
    "    # real unigram for test data\n",
    "    test_result = 0\n",
    "    for i in range(len(test_grouped_Data)):\n",
    "        # calculate naive_bayes\n",
    "        res = naive_bayes(test_grouped_Data.iloc[i].text, 1, prior_real, prior_fake, train_unigram_g0_frekans,\n",
    "                          train_unigram_g0_feature, train_unigram_g1_frekans, train_unigram_g1_feature,\n",
    "                          sum_of_train_unigram_g0_frekans, sum_of_train_unigram_g1_frekans, len_train_g0_frekans,\n",
    "                          len_train_g1_frekans)\n",
    "\n",
    "        # for real text group and unigram\n",
    "        if res == test_grouped_Data.iloc[i].label:\n",
    "            test_result += 1\n",
    "    print(\"Unigram Accuracy= %\", (test_result / len(test_grouped_Data.label)) * 100)\n",
    "\n",
    "    # real bigram for test data\n",
    "    test_result = 0\n",
    "    for i in range(len(test_grouped_Data)):\n",
    "        # calculate naive_bayes\n",
    "        res = naive_bayes(test_grouped_Data.iloc[i].text, 2, prior_real, prior_fake, train_bigram_g0_frekans,\n",
    "                          train_bigram_g0_feature, train_bigram_g1_frekans, train_bigram_g1_feature,\n",
    "                          sum_of_train_bigram_g0_frekans, sum_of_train_bigram_g1_frekans, len_train_g0_frekans,\n",
    "                          len_train_g1_frekans)\n",
    "\n",
    "        # for real text group and unigram\n",
    "        if res == test_grouped_Data.iloc[i].label:\n",
    "            test_result += 1\n",
    "    print(\"Bigram Accuracy= %\", (test_result / len(test_grouped_Data.label)) * 100)\n",
    "\n",
    "#               test data,ng number,prior real,prior fake | unigram or bigram and fake or real features\n",
    "def naive_bayes(data_text, ng_number, prior_real, prior_fake, train_g0_frekans, train_g0_feature, train_g1_frekans,\n",
    "                train_g1_feature, All_train_g0_frekans, All_train_g1_frekans, len_train_g0_feature, len_train_g1_feature):\n",
    "\n",
    "    test = n_gram_function2([data_text], ng_number)\n",
    "\n",
    "    if type(test) != type(list()):\n",
    "        test_frekans = test.sum(axis=0)\n",
    "        test_unigram_feature = n_gram.get_feature_names()\n",
    "\n",
    "        p_fake = 0\n",
    "        p_real = 0\n",
    "        for j in range(len(test_unigram_feature)):\n",
    "            # test words\n",
    "            index1 = test_unigram_feature[j]\n",
    "            # frequency of test word\n",
    "            freq = test_frekans.item(j)\n",
    "            # real value\n",
    "            try:\n",
    "                # test word index\n",
    "                real_index = train_g0_feature.index(index1)\n",
    "                p_real += freq*np.log10(sum_frequency(train_g0_frekans.item(real_index), All_train_g0_frekans, len_train_g0_feature))\n",
    "            except:\n",
    "                pass\n",
    "            # fake value\n",
    "            try:\n",
    "                # test word index\n",
    "                fake_index = train_g1_feature.index(index1)\n",
    "                p_fake += freq*np.log10(sum_frequency(train_g1_frekans.item(fake_index), All_train_g1_frekans, len_train_g1_feature))\n",
    "            except:\n",
    "                pass\n",
    "        # effect of priors\n",
    "        p_real += np.log10(prior_real)\n",
    "        p_fake += np.log10(prior_fake)\n",
    "        return 1 if p_fake > p_real else 0\n",
    "\n",
    "# print presence most strongly predicts or absence most strongly predicts\n",
    "def find_n_number(frequency, text):\n",
    "    # ascending sort\n",
    "    sorted_unigram = np.argsort(frequency, kind='heapsort', axis=None)\n",
    "    print()\n",
    "    print(\"List the 10 words whose presence most strongly predicts that the news is\", text + \".\")\n",
    "    for i in sorted_unigram[-10:]:\n",
    "        print(n_gram.get_feature_names()[i], end=\" \")\n",
    "    print(\"\\n\")\n",
    "    print(\"List the 10 words whose absence most strongly predicts that the news is\", text + \".\")\n",
    "    for i in sorted_unigram[:10]:\n",
    "        print(n_gram.get_feature_names()[i], end=\" \")\n",
    "    print()\n",
    "\n",
    "\n",
    "def part3(data):\n",
    "\n",
    "    grouped_data = data[[\"text\", \"label\"]]\n",
    "    # 0 is real 1 is fake\n",
    "    group0 = grouped_data[grouped_data.label == 0]\n",
    "    group1 = grouped_data[grouped_data.label == 1]\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "    unigram_g0 = n_gram_function(group0, 1, \"text\")\n",
    "    counts_unigram_g0 = unigram_g0.toarray()\n",
    "    # Each row is normalized to have unit Euclidean norm\n",
    "    tfidf_group0 = transformer.fit_transform(counts_unigram_g0)\n",
    "    find_n_number(transformer.idf_, \"real\")\n",
    "\n",
    "\n",
    "    unigram_g1 = n_gram_function(group1, 1, \"text\")\n",
    "    counts_unigram_g1 = unigram_g1.toarray()\n",
    "    # Each row is normalized to have unit Euclidean norm\n",
    "    tfidf_group1 = transformer.fit_transform(counts_unigram_g1)\n",
    "    find_n_number(transformer.idf_, \"fake\")\n",
    "\n",
    "def main():\n",
    "    # Handle Data\n",
    "    fake_news_train = pd.read_csv(\"fake_news_train.csv\", delimiter=',', encoding=\"utf8\")\n",
    "\n",
    "    # block nan values and empty lines\n",
    "    fake_news_train = fake_news_train.dropna()\n",
    "    fake_news_train = fake_news_train[fake_news_train.text != \" \"]\n",
    "    fake_news_train = fake_news_train[fake_news_train.text != \"  \"]\n",
    "\n",
    "    #clock\n",
    "    start = datetime.datetime.now()\n",
    "    part1(fake_news_train)\n",
    "    finish = datetime.datetime.now()\n",
    "    print(\"\\npart1 finished \", finish-start, \" ms\", (finish-start).seconds, \" second\\n\")\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    part2(fake_news_train)\n",
    "    finish = datetime.datetime.now()\n",
    "    print(\"\\npart2 finished \", finish-start, \" ms\", (finish-start).seconds, \" second\")\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    part3(fake_news_train)\n",
    "    finish = datetime.datetime.now()\n",
    "    print(\"\\npart3 finished \", finish - start, \" ms\", (finish - start).seconds, \" second\")\n",
    "\n",
    "main()\n",
    "# Program and Timer end\n",
    "b = datetime.datetime.now()\n",
    "print(\"\\nprogram finished\", b - a, \" ms\", (b - a).seconds, \" second\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Theory Question\n",
    "\n",
    "#Question 1\n",
    "\n",
    "#Question 2  Fill the blanks with T (True) or F (False) for the statements given above:\n",
    "\n",
    "1- T\n",
    "2- F\n",
    "\n",
    "#Naive Bayes\n",
    "1- Answer is 1/2\n",
    "\n",
    "P(C = 1|x = 1, y = 1, z = 0) = P(C = 1|x = 1, y = 1, z = 0)/P(x = 1, y = 1, z = 0)\n",
    "\n",
    "= P(C = 1).P(x = 1|C =1).P(y = 1|C =1).P(z = 1|C =1)/ P(C = 1,x = 1, y = 1, z = 0) + P(C = 0,x = 1, y = 1, z = 0)\n",
    "\n",
    "2- Answer is 2/3\n",
    "\n",
    "P(C = 0|x = 1, y = 1) = P(C = 0|x = 1, y = 1) / P(x = 1, y = 1)\n",
    "\n",
    "= P(C=0).P(x = 1 | C = 0). P(y = 1 | C = 0) / P(x = 1, y = 1, C = 1) + P(x = 1, y = 1, C = 0)\n",
    "\n",
    "#Joint Naive Bayes\n",
    "\n",
    "1- Answer is 0\n",
    "\n",
    "Let num(X) be the number of records in our data matching X. Then we have P(C = 1|x = 1, y = 1, z=0) = num(P(C = 1|x = 1, y = 1, z = 0)) / num(P(x = 1, y = 1, z = 0))\n",
    "\n",
    "2- Answer is 1/2\n",
    "\n",
    "P(C = 0|x = 1, y = 1) = num(P(C = 0|x = 1, y = 1)) / num(P(x = 1, y = 1))\n",
    "\n",
    "#Last question : Assume that you have three variables, which are A, B and C.\n",
    "\n",
    "1- Answer is \"Not enough info.\"\n",
    "\n",
    "2- Answer is \"Not enough info.\"\n",
    "\n",
    "3- P(C | A,B) = P( C | A) since P(B) = 1. in this case P(C | A,B) = P( C | A)/ P(A) = 0.2/0.3 = 2/3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis of the results for prediction:\n",
    "Train data = %80 of All data (except test data)\n",
    "\n",
    "Test data = %20 of All data randomly\n",
    "\n",
    "# Test 1 min_df = 40\n",
    "\n",
    "Specific 3 title keywords of unigram real group\n",
    "\n",
    "new = % 0.38052466495580267 york = % 0.36070715711434276 times = % 0.35785571713715425\n",
    "\n",
    "Specific 3 title keywords of bigram real group\n",
    "\n",
    "new york = % 55.604395604395606 york times = % 54.78021978021978 donald trump = % 5.428571428571429\n",
    "\n",
    "Specific 3 title keywords of unigram fake group\n",
    "\n",
    "trump = % 0.1772893772893773 hillary = % 0.1304029304029304 clinton = % 0.11025641025641025\n",
    "\n",
    "Specific 3 title keywords of bigram fake group\n",
    "\n",
    "hillary clinton = % 22.2 donald trump = % 16.8 world war = % 6.8\n",
    "\n",
    "part1 finished  0:00:00.793547  ms 0  second\n",
    "\n",
    "Unigram Accuracy= % 37.856406733081414\n",
    "\n",
    "Bigram Accuracy= % 21.642047406389555\n",
    "\n",
    "part2 finished  0:04:53.469783  ms 293  second\n",
    "\n",
    "List the 10 words whose presence most strongly predicts that the news is real.\n",
    "\n",
    "bite bisexual transmitted towering cramped cracks implementing imperative immunity illustrates \n",
    "\n",
    "List the 10 words whose absence most strongly predicts that the news is real.\n",
    "\n",
    "said new people like time president just mr years did \n",
    "\n",
    "List the 10 words whose presence most strongly predicts that the news is fake.\n",
    "\n",
    "attendance preaching pr 7th placement photographs photographer persuade euros adverse \n",
    "\n",
    "List the 10 words whose absence most strongly predicts that the news is fake.\n",
    "\n",
    "2016 people just like time said new clinton world hillary \n",
    "\n",
    "part3 finished  0:00:16.890267  ms 16  second\n",
    "\n",
    "program finished 0:05:12.186351  ms 312  second\n",
    "\n",
    "# Test 2 min_df = 35\n",
    "\n",
    "min_df = 35\n",
    "\n",
    "Specific 3 title keywords of unigram real group\n",
    "\n",
    "new = % 0.22361861673159902 york = % 0.2119726865233966 times = % 0.21029701311214444\n",
    "\n",
    "Specific 3 title keywords of bigram real group\n",
    "\n",
    "new york = % 29.5906432748538 york times = % 29.15204678362573 donald trump = % 2.888888888888889\n",
    "\n",
    "Specific 3 title keywords of unigram fake group\n",
    "\n",
    "trump = % 0.09401709401709402 hillary = % 0.06915306915306915 clinton = % 0.05846930846930847\n",
    "\n",
    "Specific 3 title keywords of bigram fake group\n",
    "\n",
    "hillary clinton = % 14.8 donald trump = % 11.2 world war = % 4.533333333333333\n",
    "\n",
    "part1 finished  0:00:01.104459  ms 1  second\n",
    "\n",
    "Unigram Accuracy = % 37.341119890072136\n",
    "\n",
    "Bigram Accuracy % 21.229817931982137\n",
    "\n",
    "part2 finished  0:06:25.925290  ms 385  second\n",
    "\n",
    "List the 10 words whose presence most strongly predicts that the news is real.\n",
    "\n",
    "spiked alienate sped specialty begging southwestern halloween socks beautifully accelerating \n",
    "\n",
    "List the 10 words whose absence most strongly predicts that the news is real.\n",
    "\n",
    "said new people like time president just mr years did \n",
    "\n",
    "List the 10 words whose presence most strongly predicts that the news is fake.\n",
    "\n",
    "adler noch censored negatively navigate muscles mud du minimize 1964 \n",
    "\n",
    "List the 10 words whose absence most strongly predicts that the news is fake.\n",
    "\n",
    "2016 people just like time said new clinton world hillary \n",
    "\n",
    "part3 finished  0:00:18.166663  ms 18  second\n",
    "\n",
    "program finished 0:06:46.244138  ms 406  second\n",
    "\n",
    "\n",
    "\n",
    "# Test3 min_df = 30\n",
    "\n",
    "min_df = 30\n",
    "\n",
    "Specific 3 title keywords of unigram real group\n",
    "\n",
    "new = % 0.13472653390878575 york = % 0.1277100527498044 times = % 0.1267004871154186\n",
    "\n",
    "Specific 3 title keywords of bigram real group\n",
    "\n",
    "new york = % 26.63157894736842 york times = % 26.236842105263158 donald trump = % 2.6\n",
    "\n",
    "Specific 3 title keywords of unigram fake group\n",
    "\n",
    "trump = % 0.059422958870472685 hillary = % 0.04370779619398404 clinton = % 0.03695518723143033\n",
    "\n",
    "Specific 3 title keywords of bigram fake group\n",
    "\n",
    "hillary clinton = % 7.928571428571429 donald trump = % 6.0 world war = % 2.4285714285714284\n",
    "\n",
    "part1 finished  0:00:00.785576  ms 0  second\n",
    "\n",
    "Unigram Accuracy = % 37.272414977670906\n",
    "\n",
    "Bigram Accuracy % 20.26794915836482\n",
    "\n",
    "part2 finished  0:07:10.114175  ms 430  second\n",
    "\n",
    "List the 10 words whose presence most strongly predicts that the news is real.\n",
    "\n",
    "reynolds revered commonwealth commodities retains forums bald restriction airing resistant \n",
    "\n",
    "List the 10 words whose absence most strongly predicts that the news is real.\n",
    "\n",
    "said new people like time president just mr years did \n",
    "\n",
    "List the 10 words whose presence most strongly predicts that the news is fake.\n",
    "\n",
    "liver dilemma digg bs dictators deutsch destined designs derail anticipating \n",
    "\n",
    "List the 10 words whose absence most strongly predicts that the news is fake.\n",
    "\n",
    "2016 people just like time said new clinton world hillary \n",
    "\n",
    "part3 finished  0:00:18.305029  ms 18  second\n",
    "\n",
    "program finished 0:07:30.219503  ms 450  second\n",
    "\n",
    "\n",
    "\n",
    "# Test4 min_df = 25\n",
    "\n",
    "Specific 3 title keywords of unigram real group\n",
    "\n",
    "new = % 0.07694081697368042 york = % 0.07293378304361613 times = % 0.07235723139900256\n",
    "\n",
    "Specific 3 title keywords of bigram real group\n",
    "\n",
    "new york = % 12.463054187192117 york times = % 12.27832512315271 donald trump = % 1.2167487684729064\n",
    "\n",
    "Specific 3 title keywords of unigram fake group\n",
    "\n",
    "trump = % 0.0346134592004577 hillary = % 0.025459486519344918 clinton = % 0.02152613888292927\n",
    "\n",
    "Specific 3 title keywords of bigram fake group\n",
    "\n",
    "hillary clinton = % 2.4395604395604398 donald trump = % 1.8461538461538463 world war = % 0.7472527472527473\n",
    "\n",
    "part1 finished  0:00:00.800569  ms 0  second\n",
    "\n",
    "Unigram Accuracy = % 36.79148059086225\n",
    "\n",
    "Bigram Accuracy = % 19.99312950875988\n",
    "\n",
    "part2 finished  0:08:37.182650  ms 517  second\n",
    "\n",
    "List the 10 words whose presence most strongly predicts that the news is real.\n",
    "\n",
    "predictably expedited precinct churchill chrysler pounding exhausting christina exemption 17th \n",
    "\n",
    "List the 10 words whose absence most strongly predicts that the news is real.\n",
    "\n",
    "said new people like time president just mr years did \n",
    "\n",
    "List the 10 words whose presence most strongly predicts that the news is fake.\n",
    "\n",
    "dairy indonesia indications toys tickets thwart thrust thinker theoretically theoretical \n",
    "\n",
    "List the 10 words whose absence most strongly predicts that the news is fake.\n",
    "\n",
    "2016 people just like time said new clinton world hillary \n",
    "\n",
    "part3 finished  0:00:18.297166  ms 18  second\n",
    "\n",
    "program finished 0:11:13.296029  ms 550  second\n",
    "\n",
    "\n",
    "\n",
    "# Test5 min_df = 20\n",
    "\n",
    "Specific 3 title keywords of unigram real group\n",
    "\n",
    "new = % 0.0386601484700344 york = % 0.03664674995473475 times = % 0.036357052326634075\n",
    "\n",
    "Specific 3 title keywords of bigram real group\n",
    "\n",
    "new york = % 4.48581560283688 york times = % 4.419326241134752 donald trump = % 0.4379432624113475\n",
    "\n",
    "Specific 3 title keywords of unigram fake group\n",
    "\n",
    "trump = % 0.0140277657015332 hillary = % 0.010317943367243428 clinton = % 0.008723879082978291\n",
    "\n",
    "Specific 3 title keywords of bigram fake group\n",
    "\n",
    "hillary clinton = % 0.8774703557312253 donald trump = % 0.6640316205533597 world war = % 0.26877470355731226\n",
    "\n",
    "part1 finished  0:00:00.844603  ms 0  second\n",
    "\n",
    "Unigram Accuracy = % 37.272414977670906\n",
    "\n",
    "Bigram Accuracy = % 20.199244245963584\n",
    "\n",
    "part2 finished  0:12:26.105767  ms 746  second\n",
    "\n",
    "List the 10 words whose presence most strongly predicts that the news is real.\n",
    "\n",
    "namesake mutilation mustache mushrooms mumbai captivity ashraf multiculturalism motel 112 \n",
    "\n",
    "List the 10 words whose absence most strongly predicts that the news is real.\n",
    "\n",
    "said new people like time president just mr years did \n",
    "\n",
    "List the 10 words whose presence most strongly predicts that the news is fake.\n",
    "\n",
    "confrontations salvage gambling conformity rwanda functional fucked rockets absorption concession \n",
    "\n",
    "List the 10 words whose absence most strongly predicts that the news is fake.\n",
    "\n",
    "2016 people just like time said new clinton world hillary \n",
    "\n",
    "part3 finished  0:00:22.319487  ms 22  second\n",
    "\n",
    "program finished 0:12:50.309560  ms 710  second\n",
    "\n",
    "\n",
    "\n",
    "# Test6 min_df = 15\n",
    "\n",
    "min_df = 15\n",
    "\n",
    "Specific 3 title keywords of unigram real group\n",
    "\n",
    "new = % 0.017346938775510204 york = % 0.01644352008319251 times = % 0.016313531782139606\n",
    "\n",
    "Specific 3 title keywords of bigram real group\n",
    "\n",
    "new york = % 2.156862745098039 york times = % 2.1248934356351237 donald trump = % 0.21057118499573743\n",
    "\n",
    "Specific 3 title keywords of unigram fake group\n",
    "\n",
    "trump = % 0.006296549256838066 hillary = % 0.004631346147591635 clinton = % 0.003915829186587309\n",
    "\n",
    "Specific 3 title keywords of bigram fake group\n",
    "\n",
    "hillary clinton = % 0.2707317073170732 donald trump = % 0.2048780487804878 world war = % 0.08292682926829269\n",
    "\n",
    "part1 finished  0:00:00.800575  ms 0  second\n",
    "\n",
    "Unigram Accuracy = % 35.692201992442456\n",
    "\n",
    "Bigram Accuracy = % 18.85949845413947\n",
    "\n",
    "part2 finished  0:15:35.877770  ms 935  second\n",
    "\n",
    "List the 10 words whose presence most strongly predicts that the news is real.\n",
    "\n",
    "julio brat ya joyous wrest adel worsen workouts johnston 123 \n",
    "\n",
    "List the 10 words whose absence most strongly predicts that the news is real.\n",
    "\n",
    "said new people like time president just mr years did \n",
    "\n",
    "List the 10 words whose presence most strongly predicts that the news is fake.\n",
    "\n",
    "pbs cherished pawns patriarchal astonishingly assuring parkinson paltry pad packaging \n",
    "\n",
    "List the 10 words whose absence most strongly predicts that the news is fake.\n",
    "\n",
    "2016 people just like time said new clinton world hillary \n",
    "\n",
    "part3 finished  0:00:21.123794  ms 21  second\n",
    "\n",
    "program finished 0:15:58.831861  ms 958  second"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some Comments\n",
    "\n",
    "Increasing the number of min_df sometimes affects the results positively\n",
    "\n",
    "Increasing min_df or decreasing max_df reduces program runtime\n",
    "\n",
    "bigram results lower than unigram results\n",
    "\n",
    "In part3, when the results are calculated from the total frequency of words, the result is 1 for all words. In this case does not work well"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}