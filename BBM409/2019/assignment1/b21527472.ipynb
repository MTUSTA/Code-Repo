{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# Timer starts\n",
    "a = datetime.datetime.now()\n",
    "\n",
    "# compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\n",
    "def cosine_similarity(v1, v2):\n",
    "    cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    return cos_sim\n",
    "\n",
    "# Simple error\n",
    "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    return actual - predicted\n",
    "\n",
    "# calculate Mean Absolute Error\n",
    "def mae(actual: np.ndarray, predicted: np.ndarray):\n",
    "    return np.mean(np.abs(_error(actual, predicted)))\n",
    "\n",
    "# create similarity matrix\n",
    "def pandas_sim(train_data, test_data):\n",
    "    # fills similarity matrix pandas dataframe\n",
    "    sim_matrix_df = pd.DataFrame(0, index=test_data.index, columns=train_data.index, dtype=float)\n",
    "    for row in range(test_data.__len__()):\n",
    "        for column in range(train_data.__len__()):\n",
    "            sim_matrix_df.iat[row, column] = cosine_similarity(test_data.iloc[row].to_numpy(), train_data.iloc[column].to_numpy())\n",
    "    # from pandas to numpy\n",
    "    numpy_sim_matrix_df = sim_matrix_df.to_numpy()\n",
    "    return numpy_sim_matrix_df\n",
    "\n",
    "# Weighted k-NN algorithm\n",
    "def weighted(user_based_table, train_data, test_data,numpy_sim_matrix_df, sorted_sim_matrix, k):\n",
    "    averagemean_error = 0.0\n",
    "    # find neigbours and calculate average mean error\n",
    "    for i in range(len(sorted_sim_matrix)):\n",
    "        total = np.zeros(user_based_table.keys().__len__(), dtype=float)\n",
    "        # and select first k distances\n",
    "        total_weight = 0\n",
    "        for j in range(k):\n",
    "            weight = 1/(1-(numpy_sim_matrix_df[i][sorted_sim_matrix[i][-j - 1]]))\n",
    "            total = np.add(total, np.multiply(train_data.values[sorted_sim_matrix[i][-j - 1]], weight))\n",
    "            total_weight += weight\n",
    "        # average of the distances\n",
    "        total = np.divide(total, total_weight)\n",
    "        averagemean_error += mae(test_data.values[i], total)\n",
    "    # converts % error\n",
    "    averagemean_error = averagemean_error / len(sorted_sim_matrix) / 5 * 100\n",
    "    return averagemean_error\n",
    "\n",
    "# K-NN algorithm\n",
    "def calc_mae(user_based_table, train_data, test_data, sorted_sim_matrix, k):\n",
    "    averagemean_error = 0.0\n",
    "    # find neigbours and calculate average mean error\n",
    "    for i in range(len(sorted_sim_matrix)):\n",
    "        total = np.zeros(user_based_table.keys().__len__(), dtype=float)\n",
    "        # and select first k distances\n",
    "        for j in range(k):\n",
    "            total = np.add(total, train_data.values[sorted_sim_matrix[i][-j - 1]])\n",
    "        # average of the distances\n",
    "        total = np.divide(total, k)\n",
    "        averagemean_error += mae(test_data.values[i], total)\n",
    "    # converts % error\n",
    "    averagemean_error = averagemean_error / len(sorted_sim_matrix) / 5 * 100\n",
    "    return averagemean_error\n",
    "# program starts\n",
    "def main():\n",
    "\n",
    "    # Handle Data\n",
    "    ratings = pd.read_csv(\"ratings_train.csv\", delimiter=',', encoding=\"utf8\")\n",
    "    user_based_table = pd.pivot_table(ratings, values=\"rating\", index='userId', columns='movieId')\n",
    "    # calculate average and fill for blocked NAN and similarity\n",
    "    user_based_table = user_based_table.T.fillna(user_based_table.mean(axis=1)).T\n",
    "    # k fold cross validation\n",
    "    k = 7 # neighbors number\n",
    "    k_fold_cross_value = 40\n",
    "    piece = user_based_table.index.size // k_fold_cross_value # lenght of the every test piece\n",
    "    # empty list for\n",
    "    Avg_mean_error_list = [[], []]\n",
    "    for i in range(1, k_fold_cross_value + 1):\n",
    "        # split test data\n",
    "        test_data = user_based_table[(i - 1) * piece:i * piece]\n",
    "        # previously used test data\n",
    "        piece_before_test_data = user_based_table[:(i - 1) * piece]\n",
    "        # data -> after test piece\n",
    "        piece_after_test_data = user_based_table[i * piece:]\n",
    "        # combine piece_before_test_data and piece_after_test_data\n",
    "        frames = [piece_before_test_data, piece_after_test_data]\n",
    "        # train data\n",
    "        train_data = pd.concat(frames)\n",
    "        # numpy similarity matrix\n",
    "        numpy_sim_matrix_df = pandas_sim(train_data, test_data)\n",
    "        # output sorted array indices\n",
    "        sorted_sim_matrix = np.argsort(numpy_sim_matrix_df, kind='heapsort', axis=1)\n",
    "        b_timer = datetime.datetime.now()\n",
    "        # add list calculated average MSE for k-NN\n",
    "        Avg_mean_error_list[0].append(calc_mae(user_based_table, train_data, test_data, sorted_sim_matrix, k))\n",
    "        c_timer = datetime.datetime.now()\n",
    "        print(\"Test piece group=\", i, \"k-NN finished MAE: %\", Avg_mean_error_list[0][i-1],\"calculation time:\", c_timer-b_timer , \" ms\", (c_timer-b_timer).seconds, \" second\")\n",
    "        b_timer = datetime.datetime.now()\n",
    "        # add list calculated average MSE for Weighted k-NN\n",
    "        Avg_mean_error_list[1].append(weighted(user_based_table, train_data, test_data, numpy_sim_matrix_df, sorted_sim_matrix, k))\n",
    "        c_timer = datetime.datetime.now()\n",
    "        print(\"Test piece group=\", i, \"Weighted k-NN finished MAE: %\", Avg_mean_error_list[1][i-1],\"calculation time:\", c_timer-b_timer, \" ms\", (c_timer-b_timer).seconds, \" second\")\n",
    "\n",
    "    print(\"Average k-NN MAE: %\", np.average(Avg_mean_error_list[0]))\n",
    "    print(\"Average Weighted k-NN MAE: %\", np.average(Avg_mean_error_list[1]))\n",
    "    y_limit = round(np.max(Avg_mean_error_list))+5\n",
    "\n",
    "    plt.plot(Avg_mean_error_list[0], \"ro\", Avg_mean_error_list[1], \"bs\")\n",
    "    plt.axis([0, k_fold_cross_value, 0, y_limit])\n",
    "    plt.ylabel('percent numbers')\n",
    "    plt.xlabel(\"Every pieces MAE errors\")\n",
    "    plt.legend((\"k-NN MAE\", \"Weighted k-NN MAE\"))\n",
    "    plt.show()\n",
    "\n",
    "# program starts\n",
    "main()\n",
    "# Program and Timer end\n",
    "b = datetime.datetime.now()\n",
    "print(\"finished\", b - a, \" ms\", (b - a).seconds, \" second\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Test piece group= 1 k-NN finished MAE 0:00:00.024019  ms 0  second\nTest piece group= 1 Weighted k-NN finished MAE 0:00:00.022017  ms 0  second\n",
      "Test piece group= 2 k-NN finished MAE 0:00:00.009007  ms 0  second\nTest piece group= 2 Weighted k-NN finished MAE 0:00:00.012024  ms 0  second\n",
      "Test piece group= 3 k-NN finished MAE 0:00:00.011009  ms 0  second\nTest piece group= 3 Weighted k-NN finished MAE 0:00:00.015007  ms 0  second\n",
      "Test piece group= 4 k-NN finished MAE 0:00:00.010005  ms 0  second\nTest piece group= 4 Weighted k-NN finished MAE 0:00:00.011010  ms 0  second\n",
      "Test piece group= 5 k-NN finished MAE 0:00:00.012009  ms 0  second\nTest piece group= 5 Weighted k-NN finished MAE 0:00:00.039028  ms 0  second\n",
      "Test piece group= 6 k-NN finished MAE 0:00:00.009007  ms 0  second\nTest piece group= 6 Weighted k-NN finished MAE 0:00:00.009008  ms 0  second\n",
      "Test piece group= 7 k-NN finished MAE 0:00:00.009009  ms 0  second\nTest piece group= 7 Weighted k-NN finished MAE 0:00:00.008008  ms 0  second\n",
      "Test piece group= 8 k-NN finished MAE 0:00:00.010023  ms 0  second\nTest piece group= 8 Weighted k-NN finished MAE 0:00:00.010005  ms 0  second\n",
      "Test piece group= 9 k-NN finished MAE 0:00:00.011011  ms 0  second\nTest piece group= 9 Weighted k-NN finished MAE 0:00:00.011008  ms 0  second\n",
      "Test piece group= 10 k-NN finished MAE 0:00:00.012012  ms 0  second\nTest piece group= 10 Weighted k-NN finished MAE 0:00:00.014009  ms 0  second\nAverage k-NN MAE: 18.8718798404553\nAverage Weighted k-NN MAE: 18.926546915681786\n",
      "finished 0:01:19.623487  ms 79  second\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BBM409 Assignment 1\n",
    "#Mehmet Taha USTA B21527472\n",
    "# THEORY QUESTİON\n",
    "# k-\tNearest Neighbor Classification\n",
    "#Question1 -> dataset grows efficiency or speed of algorithm declines very fast.\n",
    "#Question1 -> K-NN is to choose the optimal number of neighbors to be consider while classifying the new data entry.\n",
    "#Question1 -> k-NN doesn’t perform well on imbalanced data. If we consider two classes, A and B, and the majority of the training data is labeled as A, then the model will ultimately give a lot of preference to A. This might result in getting the less common class B wrongly classified.\n",
    "#Question1 -> K-NN algorithm is very sensitive to outliers as it simply chose the neighbors based on distance criteria.\n",
    "\n",
    "#Question2\n",
    "#Leave-one-out cross-validation is approximately unbiased, because the difference in size between the training set used in each fold and the entire dataset is only a single pattern. \n",
    "#However, while leave-one-out cross-validation is approximately unbiased, it tends to have a high variance (so you would get very different estimates if you repeated the estimate with different initial samples of data from the same distribution). As the error of the estimator is a combination of bias and variance, whether leave-one out cross-validation is better than 10-fold cross-validation depends on both quantities.\n",
    "\n",
    "#Question3\n",
    "#k=1 -> o will be negative(-) because nearest 1 point negative(-),\n",
    "#k=3 -> o will be negative(-) because nearest 2 point negative(-), 1 point positive(+)    \n",
    "#k=5 -> o will be positive(+) because nearest 2 point negative(-), 3 point positive(+)  \n",
    "#Question4\n",
    "#-True\n",
    "#-False\n",
    "#-False\n",
    "\n",
    "# Linear Regression\n",
    "#Question1 -> x' = (x-min(x)) / ( max(x) - min(x) ) --> x' = (4489 - 2025) / (8464 - 2025) --> 2464 / 6439 = 0,3826681161671067\n",
    "#Question1 -> x' = (x-mean(x)) / ( max(x) - min(x) ) --> x' = (4489 - 4900) / (8464 - 2025) --> -411 / 6439 = - 0,0638297872340426\n",
    "\n",
    "#Question2\n",
    "#-> two methods are used for least square line fit\n",
    "#-> vertical offset is best lineer Model will have the smallest value for lineer model vector\n",
    "#-> The vertical offsets fitting is more simple and more often used method.\n",
    "#-> A condition for finding the line with the least sum of squared perpendicular distances to a point-cloud of data is that you know / assume / demand that the line passes through the centre of gravity of the point-cloud. This assumption will not be proved here but makes good sense if you think of the point-cloud as a physical body and the deduced lines as principal axis of inertia.\n",
    "\n",
    "#Question3\n",
    "#y= a+bx\n",
    "#0.5 = a + b\n",
    "#1 = a + 2b\n",
    "#2 = a + 4b\n",
    "#0 = a + 0\n",
    "#a= 0 b = 0.5\n",
    "#\n",
    "#Question4\n",
    "#It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm.\n",
    "#It makes interpretation much easier. \n",
    "#It can make the analysis of coefficients easier.\n",
    "#The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.To supress this effect, we need to bring all features to the same level of magnitudes.\n",
    "\n",
    "# Movie Recommendation System\n",
    "#Program starts by running main function\n",
    "#file (\"ratings_train.csv\") is read by pandas\n",
    "#create dataframe according to user and movies\n",
    "#The values k and k_fold_cross_value are set manually\n",
    "#The data is divided by applying k_fold_cross_validation.\n",
    "#The similarities of each test data are calculated by the pandas_sim (train_data, test_data) function\n",
    "#similarity matrix is sorted by np.argsort\n",
    "#For k-NN, the nearest neighbors will be calculated by the calc_mae (...) function. then the percentage average MSE will be calculated\n",
    "#Weighted For k-NN, the nearest neighbors will be calculated by the weighted (...) function. then the percentage average MSE will be calculated\n",
    "#Each test piece MSE results are printed to the console\n",
    "#Finally, Average k-NN MAE and Average Weighted k-NN MAE are printed to the console and graphically visualized\n",
    "#Program ends and run time is printed to the console\n",
    "\n",
    "#computer system features -> 8 gb ram, intel i5 4210U 1.7ghz to 2.4ghz (2 core 4 thread)\n",
    "#Average working time = 1min\n",
    "#k=1 k_fold_Cross_validation=10 time= 1min 2s MAE = %22.44\n",
    "#k=1 k_fold_Cross_validation=20 time= 1min 2s MAE = %22.62\n",
    "#k=1 k_fold_Cross_validation=30 time= 1min MAE = %22.63\n",
    "#k=1 k_fold_Cross_validation=40 time= 59s MAE = %22.8\n",
    "\n",
    "#k=3 k_fold_Cross_validation=10 time= 1min 1s MAE = %18.\n",
    "#k=3 k_fold_Cross_validation=20 time= 1min 4s MAE = %19.\n",
    "#k=3 k_fold_Cross_validation=30 time= 1min 2s MAE = %19.\n",
    "#k=3 k_fold_Cross_validation=40 time= 59s MAE = %19.\n",
    "\n",
    "#k=5 k_fold_Cross_validation=10 time= 58s MAE = %17.5\n",
    "#k=5 k_fold_Cross_validation=20 time= 1min MAE = %17.6\n",
    "#k=5 k_fold_Cross_validation=30 time= 1min 1s MAE = %17.7\n",
    "#k=5 k_fold_Cross_validation=40 time= 59s MAE = %17.9\n",
    "\n",
    "#k=7 k_fold_Cross_validation=10 time= 58s MAE = %16.3\n",
    "#k=7 k_fold_Cross_validation=20 time= 1min MAE = %16.5\n",
    "#k=7 k_fold_Cross_validation=30 time= 1min 1s MAE = %16.7\n",
    "#k=7 k_fold_Cross_validation=40 time= 58s MAE = %16.8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}